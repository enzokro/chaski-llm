Thank you. For more context, the code you just saw above was refactored from the original versions we worked on based on the `<CHASKI>` style. Please find attached two documents: 

```markdown
- `chaski_guide.md`.  
- `chaski_overview.md`.
```  

These describe the `<CHASKI>` style and overall state. It is based, as you can see, on a combined and refined programming philosophy distilled from:   
```markdown
- lucidrains, aka Phil Wang.  
- Jeremy Howard, from fast.ai  
- Georgi Gerganov, from the llama.cpp library
```  

Please, read and integrate the new `<CHASKI>` style. Analyze it, critique it, evaluate it. Where are its weakpoints? How could it be improved? Internally, generate a series of genetic improvements and synthesize them into an optimal approach. Then, describe your new updated version of the `<CHASKI>` style.  

`chaski_guide.md`
```
The <CHASKI> Style Guide integrates the essence of <JEREMY_HOWARD>, <LUCIDRAINS>, and <GGML> into a unified approach to software development. This comprehensive method emphasizes data-driven decision-making, modular design, clear documentation, and a strong focus on community and collaboration. Here's a detailed breakdown of each principle:

**Emphasis on Data-Driven Decision Making**  

- Experimentation and Metrics: Advocates for basing decisions on empirical data, using experimentation, performance benchmarking, and A/B testing to guide development choices.    
- User-Centric Design: Prioritizes gathering and analyzing user feedback to ensure the software meets real user needs and expectations, aligning product development with user satisfaction.   
- Iterative Improvement: Encourages the use of data to identify areas for optimization and enhancement, fostering a culture of continuous improvement.  
- Risk Management: Employs data analysis to assess potential risks and make informed decisions, minimizing the impact of unforeseen issues on project timelines and outcomes.  
  
  
**Modular and Composable Design**  

- Building for Extension: Promotes designing software components that are extensible and composable, facilitating easy integration and future expansion.  
- Decoupling and Independence: Stresses the importance of creating loosely coupled components that can operate independently, enhancing the system's flexibility and resilience.  
- Reuse and Efficiency: Encourages the development of reusable modules to reduce redundancy, speed up development, and maintain consistency across projects.  
- Design Patterns: Recommends leveraging established design patterns to solve common problems in a modular and scalable way, benefiting from the collective experience of the developer community.  


**Commitment to Clean and Transparent Documentation**  

- Living Documentation: Ensures that documentation evolves with the codebase, providing clear, up-to-date guidance and insights into the system's functionality and architecture.  
- Accessibility and Inclusivity: Aims to make documentation accessible to all potential users, regardless of their expertise level, fostering a more inclusive developer and user community.  
- Best Practices and Guidelines: Includes documentation of coding standards, best practices, and style guidelines to promote code quality and consistency.  
- Knowledge Sharing: Facilitates the sharing of knowledge and expertise through well-documented code, contributing to the professional growth of the team and the wider community.  

**Community and Collaboration Focus**  
- Openness and Sharing: Emphasizes the importance of sharing knowledge, code, and experiences within and beyond the team, adopting open-source principles whenever possible.  
- Peer Review and Feedback: Integrates regular code reviews and constructive feedback into the development process, improving code quality and fostering team cohesion.  
- Cross-Functional Collaboration: Encourages collaboration across different disciplines and areas of expertise, breaking down silos and leveraging diverse perspectives for better solutions.  
- Community Engagement: Promotes active engagement with the broader development community, contributing to open-source projects, participating in forums, and attending conferences to stay connected and informed.   


**Conclusion**

By embracing these principles, the <CHASKI> Style Guide offers a pathway to creating software that is not only functionally robust but also a testament to the collaborative, thoughtful, and dynamic nature of modern software development. This guide serves as a solid foundation for developers to build upon, ensuring their work is not only effective but also reflective of best practices and shared wisdom.
```

`chaski_overview.md`
```
The <CHASKI> style represents an evolved approach to software development, emphasizing clarity, maintainability, performance, and aesthetic elegance. This style has been honed through the synthesis of various coding philosophies, including but not limited to the intuitive and straightforward designs of Jeremy Howard's fast.ai and the innovative, modular approaches of Phil Wang's lucidrains. Below, we delve into the key principles of the <CHASKI> style, illustrated with code snippets and design strategies employed in our journey.

1. Clarity and Simplicity
Code should be as intuitive to read as plain English, with a focus on minimizing cognitive load. Names (variables, functions, classes) should be self-explanatory, and complex logic should be broken down into digestible pieces.

```python
# BAD
def f(a, b):
    return a + b if a > 10 else a - b

# GOOD
def calculate_result_based_on_condition(first_number, second_number):
    if first_number > threshold:
        return add_numbers(first_number, second_number)
    return subtract_numbers(first_number, second_number)

def add_numbers(a, b):
    return a + b

def subtract_numbers(a, b):
    return a - b

threshold = 10
```

2. Modularity and Reusability
Code should be organized in a way that promotes reuse. Functions and classes should perform one task and do it well, following the Single Responsibility Principle. Modular code not only enhances readability but also facilitates testing and maintenance.

```python
# Instead of a monolithic function:
def process_data_and_generate_report(data):
    # Process data...
    # Generate report...
    pass

# Split functionality:
def process_data(data):
    # Process data...
    pass

def generate_report(processed_data):
    # Generate report...
    pass
```

3. Aesthetic Elegance
Beyond functionality, the <CHASKI> style values the aesthetic aspect of coding. This includes proper indentation, consistent naming conventions, and thoughtful organization of code blocks. Aesthetic elegance makes the codebase inviting and improves developer engagement.  

```html
/* Consistent naming and indentation in CSS */
:root {
    --primary-color: #005792;
    --secondary-color: #e8f1f5;
}

body {
    font-family: Arial, sans-serif;
    color: var(--primary-color);
}

.button {
    background-color: var(--secondary-color);
    border: none;
}
```


4. Performance Awareness
While maintaining clarity and elegance, <CHASKI> encourages developers to be mindful of the performance implications of their code. This means choosing efficient algorithms, being aware of time and space complexity, and optimizing resource usage without compromising code quality.

```python
# Instead of inefficiently concatenating strings in a loop:
result = ""
for s in strings:
    result += s

# Use a more efficient join method:
result = "".join(strings)
```  


5. Comprehensive Documentation and Testing
Every function, class, and module should be accompanied by clear documentation that explains its purpose, inputs, outputs, and any side effects. Unit tests are equally important, ensuring the code not only works as intended but continues to do so as the system evolves.  

```python
def calculate_area_of_circle(radius):
    """
    Calculate the area of a circle given its radius.

    Parameters:
    radius (float): The radius of the circle.

    Returns:
    float: The area of the circle.
    """
    return 3.14159 * radius ** 2

# Test
assert calculate_area_of_circle(3) == 28.27431
```

6. Embracing Modern Technologies and Practices
The <CHASKI> style encourages staying up-to-date with the latest technologies and best practices. This includes leveraging modern language features, frameworks, and tools that enhance productivity, security, and the overall quality of the codebase.  

```javascript
// Use modern JavaScript features for cleaner code
const fetchData = async () => {
    try {
        const response = await fetch('https://api.example.com/data');
        const data = await response.json();
        console.log(data);
    } catch (error) {
        console.error('Fetching data failed:', error);
    }
}
```

Conclusion
The <CHASKI> style is a holistic approach to software development that balances the art and science of coding. It's about writing code that not only machines can execute efficiently but humans can read and understand effortlessly. Through the principles of clarity, modularity, aesthetic elegance, performance awareness, comprehensive documentation, and embracing modern technologies, <CHASKI> aims to elevate the craft of programming to new heights, making software development more enjoyable and productive.
```


Thank you. Now, please find a series of attached files that are concrete, practical, production-grade examples of the <CHASKI> design philosophy and coding approach. 

```markdown
- `app.py`    
- `llm_manager.py`   
- `db.py`  
- `extract.py`  
- `web_api.py`  
```

These files are used to create a workflow for interacting with LLMs. Specifically, they are used to chat/instruct an LLM, with a Retrieval-Augmented Generation (RAG) workflow. A RAG workflow embeds a series of documents into an external knowledge base. At runtime, given a new a user query, the system finds the document embeddings that most closely match the user query. Then, it inserts the relevant text from these matching embeddings into the prompt, below the user's query, and this becomes the new augmented prompt. Below are more details:

RAG Workflow Details:
```markdown
1. The user makes an initial query.  
2. Assuming there is an existing set of embeddings, we find the top-n most relevant (by cosine distance) embeddings.  
3. Then, we insert the text of these embeddings into the prompt, underneath the user's query. We also make it clear to the LLM that this is useful context, coming from a known knowledge base, to help guide its output generation.  
4. Then we end the prompt, including any final assistant/response setups as needed.  
```

Next we include a working python example that flexes all of the code we have so far. Specifically, this is for a working RAG example, though a regular chat-based example would look similar. 

RAG Example Description:
```markdown
- Initialize the LLM Manager.  
- - Internally, it managed the EmbeddingStorage and EmbeddingModel.    
- Embed a series of sample documents.   
- Save the embeddings to a file.  
- Load the embeddings from this saved file.  
- Now, use a new query, or input, that we will pass to the model.  
- Search the embeddings for the ones that are most similar to this query.  
- Add the matching text from the relevant embeddings as part of the prompt.  
- - Make sure the prompt is told this is "context" for its generation.  
- Finish the generation task, using RAG via the context-augmented prompt with the text from the most similar embeddings.
```

RAG Example Code:
```python
from figma_llm.models.llm_manager import LLMManager
from figma_llm.embeds.db import EmbeddingStorage
from figma_llm.utils.config import Config

# regular mistral instruct format
def prompt_mistral(query, *args, **kwargs):
    """Generates a prompt formatted for RAG with user query and context."""
    return f"{query}"

# Define a function to create a RAG-formatted prompt
def rag_prompt_mistral(query, context):
    """Generates a prompt formatted for RAG with user query and context."""
    return f"{query}\nContext for Instructions: ```{context}```:"


# Initialize the LLM Manager with the desired chat format
print("Initializing the LLM Manager...")
llm_manager = LLMManager(
    model_path=Config.MODEL_PATH,
    use_embeddings=True,  # Enable embedding feature
    embedding_model_info=Config.DEFAULT_EMBEDDINGS  # Use default embedding model settings
)

# Initialize an empty EmbeddingStorage
llm_manager.embedding_storage = EmbeddingStorage(Config.DEFAULT_EMBEDDINGS['file_path'])

# Embed and store a series of sample documents
sample_documents = [
    "The capital of France is Paris.",
    "The Eiffel Tower is a famous landmark in Paris.",
    "France is known for its delicious cuisine, including croissants and baguettes.",
    "The Louvre Museum in Paris houses the famous painting, the Mona Lisa.",
]

print("Embedding and storing documents...")
for doc in sample_documents:
    llm_manager.embed_and_store(doc)

# Save the embeddings to a file for persistence
print("Saving the embeddings to a file...")
llm_manager.embedding_storage._save_to_file()

# Load the embeddings from the saved file
print("Loading the embeddings from the file...")
llm_manager.embedding_storage._load_from_file()

# Define a new query to search the embeddings
query = "Tell me about a famous landmark in France."

# Search the embeddings for the top-n most similar to the query
print("Searching for similar documents...")
top_similar = llm_manager.embedding_storage.find_top_n(
    llm_manager.embedding_model.embed(query), n=2
)

# Extract the context from the top similar embeddings
context = "\n".join([text for _, _, text in top_similar])

# Generate a response using the RAG-augmented prompt
print("Generating the response using RAG...")
rag_response = llm_manager.generate_response(rag_prompt_mistral(query, context))

# Compare with a standard response without RAG
print("Generating the response without RAG...")
response = llm_manager.generate_response(prompt_mistral(query))

# Output the results
print(f"Query: {query}")
print(f"Response without RAG:\n{response}")

print(f"Context:\n{context}")
print(f"RAG-Response:\n{rag_response}")
```


## good prompts
---
<role>You are an engineering wizard, experienced at solving complex problems across various disciplines. Your knowledge is both wide and deep. You are also a great communicator, giving very thoughtful and clear advice.</role>

You provide advice in the following <response_format>:

<response_format>

<problem_overview>Overview of the problem</problem_overview>

<challenges>Key challenges in solving the problem</challenges> 

<solution1>First potential solution</solution1>

<solution2>Second potential solution</solution2>

<solution3>Third potential solution</solution3>

<solution1_analysis>Analysis of pros and cons of Solution 1</solution1_analysis>

<solution2_analysis>Analysis of pros and cons of Solution 2</solution2_analysis>  

<solution3_analysis>Analysis of pros and cons of Solution 3</solution3_analysis>

<additional_solution>An additional solution, potentially combining ideas from the other solutions or introducing new ideas</additional_solution>

<recommendation>Your final recommendation on the best approach</recommendation>

</response_format>

<response_quality>

Each section (problem_overview, challenges, solution1, solution2, solution3, solution1_analysis, solution2_analysis, solution3_analysis, additional_solution, and recommendation) should contain a minimum of four thoughtful, detailed sentences analyzing the problem and solutions in-depth. Approach this with great care — be incredibly thoughtful and accurate. Leave no stone unturned.

</response_quality>

Here is the problem I want you to solve: <problem_to_solve>{PROBLEM_HERE}</problem_to_solve>
---


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

Variables: {'$DOCUMENTS'} ************************ Prompt: Your task is to analyze a set of documents to extract key concepts and relationships between those concepts, and then use the extracted information to create a Knowledge Graph. Here are the documents to analyze: <documents> {$DOCUMENTS} </documents> Please carefully read through the provided documents. As you read, identify and extract the most important concepts discussed in the documents. Once you have finished reading, please list out the key concepts you identified, with each concept on its own line. Next, look for relationships between the concepts you extracted. Identify as many clear relationships as you can based on the information provided in the documents. Using the extracted concepts and relationships, construct a Knowledge Graph. Represent the Knowledge Graph in the following format: <KnowledgeGraph> Concept1 - relationship - Concept2 Concept2 - relationship - Concept3 ... </KnowledgeGraph> For example: <KnowledgeGraph> Dog - is a type of - Animal Cat - is a type of - Animal Dog - chases - Cat </KnowledgeGraph> If possible, also provide a graphical representation of the Knowledge Graph using ASCII art or a simple markup format. Please only use information that is explicitly stated or very strongly implied in the provided documents. Do not incorporate any outside knowledge. Provide your full analysis and the resulting Knowledge Graph inside <result> tags.


************************

Prompt:
You will now be given a document about the Figma design tool. Your goal is to build a Knowledge Graph using the concepts and relationships in the document.

Please carefully read through the provided documents. As you read, identify and extract the most
important concepts discussed in the documents. Once you have finished reading, please list out the
key concepts you identified, with each concept on its own line.

Next, look for relationships between the concepts you extracted. Identify as many clear
relationships as you can based on the information provided in the documents.

Please only use information that is explicitly stated or very strongly implied in the provided
documents. Do not incorporate any outside knowledge.

Provide your full analysis and the resulting Knowledge Graph inside <result> tags.

************************
************************

Thank you, now we are going to refactor a bit. Right now, we rely on the `llm_manager` to use both the `EmbeddingStorage` and `EmbeddingModel` class. 
    
Let's make a new `EmbeddingManager` class that uses both of these classes instead, so we can ideally use the class standalone, outside of the LLMManager. 

`EmbeddingStorage`:
```python
def generate_id(text: str) -> str:
    """Generate a unique ID for a given text."""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

class EmbeddingStorage:
    """Manages storage and retrieval of text embeddings."""

    def __init__(self, file_path: str, file_format: str = "npz"):
        self.file_path, self.file_format = file_path, file_format
        self.embeddings = {}  # Maps IDs to embeddings
        self.metadata = {}    # Maps IDs to metadata
        self._load_from_file()

    def add(self, embeddings: List[np.ndarray], metadatas: Optional[List[Dict]] = None) -> List[str]:
        """Add new embeddings with optional metadata and texts."""
        ids = []
        for i, embedding in enumerate(embeddings):
            id = generate_id(metadatas[i]['text'] if metadatas else str(embedding.tolist()))
            self.embeddings[id] = embedding
            if metadatas:
                self.metadata[id] = metadatas[i]
            ids.append(id)
        return ids

    def get(self, ids: List[str]) -> Dict[str, np.ndarray]:
        """Retrieve embeddings by IDs."""
        return {id: self.embeddings.get(id) for id in ids if id in self.embeddings}

    def update(self, id: str, embedding: Optional[np.ndarray] = None, metadata: Optional[Dict] = None, text: Optional[str] = None):
        """Update existing embedding, metadata, and text by ID."""
        if id in self.embeddings or id in self.metadata or id in self.text_mapping:
            if embedding is not None:
                self.embeddings[id] = embedding
            if metadata is not None:
                self.metadata[id].update(metadata)
        else:
            logger.warning(f"ID '{id}' not found in storage.")

    def delete(self, ids: List[str]):
        """Remove embeddings, metadata, and texts by IDs."""
        for id in ids:
            self.embeddings.pop(id, None)
            self.metadata.pop(id, None)

    def find_top_n(self, query_embedding: np.ndarray, n: int = 5, largest_first=False) -> List[Tuple[str, float, str]]:
        """Find top-n embeddings most similar to query."""
        distances = [(id, cosine(query_embedding, emb), self.text_mapping.get(id, "")) for id, emb in self.embeddings.items()]
        return sorted(distances, key=lambda x: x[1], reverse=largest_first)[:n]

    def _load_from_file(self):
        """Load stored embeddings, metadata, and text mappings."""
        try:
            if os.path.exists(self.file_path):
                self.embeddings = np.load(self.file_path + '.npz', allow_pickle=True)
                self.metadata = np.load(self.file_path + '_metadata.npz', allow_pickle=True)
        except Exception as e:
            logger.error(f"Failed to load embedding storage: {e}")

    def _save_to_file(self):
        """Save current state to file."""
        np.savez(self.file_path + f".{self.file_format}", **self.embeddings)
        np.savez(self.file_path + f'_metadata.{self.file_format}', **self.metadata)
```

`EmbeddingModel`:  
```python
class EmbeddingModel:
    """Handles extraction of text embeddings using various libraries."""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", model_library: str = "sentence-transformers", **kwargs):
        """
        Initializes the embedding model based on the specified library and model name.
        
        Args:
            model_name (str): Identifier for the model.
            model_library (str): The library from which to load the model ('sentence-transformers', 'transformers', or 'llama').
        """
        self.model_name = model_name
        self.model_library = model_library
        self.model, self.tokenizer = self._load_model()

    def _load_model(self) -> Tuple[Optional[object], Optional[object]]:
        """Loads the model and tokenizer based on the model library."""
        if self.model_library == "sentence-transformers":
            return SentenceTransformer(self.model_name), None
        elif self.model_library == "transformers":
            return AutoModel.from_pretrained(self.model_name), AutoTokenizer.from_pretrained(self.model_name)
        elif self.model_library == "llama":
            return Llama(model_path=self.model_name, embedding=True), None
        else:
            raise ValueError(f"Unsupported model library: {self.model_library}")

    def embed(self, texts: List[str]) -> List[List[float]]:
        """Generates embeddings for a list of texts."""
        if self.model_library == "sentence-transformers":
            return self.model.encode(texts, convert_to_tensor=False).tolist()
        elif self.model_library == "transformers":
            encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            with torch.no_grad():
                model_output = self.model(**encoded_input)
            # TODO: support different poolings
            return _mean_pooling(model_output, encoded_input['attention_mask']).tolist()
        elif self.model_library == "llama":
            return [self.model.embed(text) for text in texts]
        else:
            raise ValueError(f"Unsupported model library for embedding: {self.model_library}")
```

Here is the functionality we want to manage under the new `EmbeddingManager`:

`TO_REFACTOR`:
```python
class FigmaLLM: # <-- to be refactored
    def __init__(
            self,
            embedding_model_info: Optional[Dict[str, Any]] = Config.DEFAULT_EMBEDDINGS,
            **kwargs,
        ):
        # Initialize the embedding components
        self.init_embeddings(embedding_model_info)

    @exception_handler
    def embed_and_store(self, text: str, **kwargs):
        """Chunk text, extract, and store embeddings."""
        if not hasattr(self, 'embedding_model'):
            raise ValueError("Embedding model is not initialized.")
        chunks = chunk_text(text)
        embeddings = [self.embedding_model.embed(chunk) for chunk in chunks]
        metadatas = [{"chunk_index": i, "text": chunk} for i, chunk in enumerate(chunks)]
        self.embedding_storage.add(embeddings=embeddings, metadatas=metadatas)
```

Please refactor instead to an EmbeddingEngine, that still separates the duties (aka creates its own member variables for EmbeddingModel and EmbeddingStorage, and uses these directly). We still want the storage and extraction to be isolated to their own files. For example, we have one class now that is both loading from file and loading the model. That's a bit messy for a single __init__(*args, **kwargs), 

The EmbeddingEngine is what LLMFigma should use internally, instead of the two separate classes.  