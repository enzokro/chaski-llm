{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM EMBEDS: True\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/cck/projects/chaski-llm/chaski/models/sciphi-self-rag-mistral-7b-32k.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = sciphi_sciphi-self-rag-mistral-7b-32k\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32015\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 275/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = sciphi_sciphi-self-rag-mistral-7b-32k\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32015 '<pad>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.45 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the LLM Manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '32015', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'sciphi_sciphi-self-rag-mistral-7b-32k'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from chaski.models.llm import LLM\n",
    "from chaski.utils.config import Config\n",
    "from chaski.utils.path_utils import get_outputs_dir\n",
    "\n",
    "# regular mistral instruct format\n",
    "def sci_prompt(query, *args, **kwargs):\n",
    "    return f\"\"\"### System:\n",
    "{Config.SYSTEM_MESSAGE}\n",
    "\n",
    "### Instruction:\n",
    "{query}\n",
    "\n",
    "### Response:\n",
    "    \"\"\"\n",
    "\n",
    "# Define a function to create a RAG-formatted prompt\n",
    "def rag_prompt_sci(query, context):\n",
    "    \"\"\"Generates a prompt formatted for RAG with user query and context.\"\"\"\n",
    "    return f\"\"\"### System:\n",
    "{Config.SYSTEM_MESSAGE}\n",
    "\n",
    "### Instruction:\n",
    "{query}\n",
    "\n",
    "### Response:\n",
    "{context}\n",
    "    \"\"\"\n",
    "\n",
    "# regular mistral instruct format\n",
    "def prompt_mistral(query, *args, **kwargs):\n",
    "    \"\"\"Generates a prompt formatted for RAG with user query and context.\"\"\"\n",
    "    return f\"{query}\"\n",
    "\n",
    "# Define a function to create a RAG-formatted prompt\n",
    "def rag_prompt_mistral(query, context):\n",
    "    \"\"\"Generates a prompt formatted for RAG with user query and context.\"\"\"\n",
    "    return f\"{query}, with the following context: ```{context}```\\n\"\n",
    "\n",
    "\n",
    "# Where to save the output embeddings\n",
    "out_dir = get_outputs_dir() / 'blender_embeddings'\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "file_path = out_dir / \"embeddings_v1\"\n",
    "\n",
    "\n",
    "# Initialize the LLM\n",
    "print(\"Initializing the LLM Manager...\")\n",
    "llm = LLM(\n",
    "    # model_path=Config.MODEL_PATH,\n",
    "    model_path=\"/Users/cck/projects/chaski-llm/chaski/models/sciphi-self-rag-mistral-7b-32k.Q4_K_M.gguf\",\n",
    "    use_embeddings=True,  # Enable embeddings\n",
    "    embedding_model_info=Config.DEFAULT_EMBEDDINGS,  # Use default embedding settings\n",
    ")\n",
    "\n",
    "# read in the chunks\n",
    "import pickle\n",
    "with open('chunks_v1.pkl', 'rb') as f:\n",
    "    chunked_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and storing documents...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Embed and store the sample documents\n",
    "print(\"Embedding and storing documents...\")\n",
    "for key, chunks in chunked_data.items():\n",
    "    for chunk in chunks:\n",
    "        llm.embed_and_store(chunk, do_chunk=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save the embeddings to a file for persistence\n",
    "# print(\"Saving the embeddings to a file...\")\n",
    "# llm.embeds.save_to_file(file_path)\n",
    "\n",
    "# # Load the embeddings from the saved file\n",
    "# print(\"Loading the embeddings from the file...\")\n",
    "# llm.embeds.load_from_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for similar documents...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a new query to search the embeddings\n",
    "query = \"Tell me about the MeasureIt addon in Blender\"\n",
    "\n",
    "# Search the embeddings for the top-n most similar to the query\n",
    "print(\"Searching for similar documents...\")\n",
    "top_n = 2\n",
    "top_similar = llm.embeds.find_top_n(query, n=top_n)\n",
    "\n",
    "# Extract the context from the top similar embeddings\n",
    "context = \"\\n\".join([text for _, _, text in top_similar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aabc7c21c44a171917d828363ff3bd3fa114a583190f110314c4acc58e418541',\n",
       "  0.3446535649865773,\n",
       "  '* [Add-ons](../index.html)\\n* [3D View](index.html)\\n* MeasureIt\\n# MeasureIt\\nMeasureIt is an add-on designed for displaying measures in the viewport,\\nmaking the process of design objects with exact measures, easier. These tools\\nare extremely useful for any job that requires exact measurements, including\\narchitectural projects, technical design and 3D printing.\\n## Activation\\n* Open Blender and go to Preferences then the Add-ons tab.\\n* Click 3D View then MeasureIt to enable the script.\\n## Interface\\n### Overview\\nLocated in the 3D Viewport ‣ Sidebar ‣ View tab. The MeasureIt Tools panel is\\ndescribed below.\\nTo view the measures you need to press the _Show_ button. Many measure styles\\nappear grayed out in the menu, these are active in Edit Mode.\\n* The Mesh Debug sub panel has extra display options.\\n* The Items sub panel appears after adding a measure. This contains the color settings for each measure.\\n* The Configuration sub panel contains the font settings.'),\n",
       " ('e0bba3cb89758bad630e65627e641a46baca6fceedf20011101082ee6ac45d02',\n",
       "  0.41356205809410884,\n",
       "  'As all measure definitions are saved in the blend-file, you can save the file\\nand the next time you use it, the measures will be ready.\\nReference\\nCategory:\\n3D View\\nDescription:\\nTools for measuring objects in the 3D Viewport.\\nLocation:\\n3D Viewport ‣ Sidebar ‣ View tab\\nFile:\\nmeasureit folder\\nAuthor:\\nAntonio Vazquez (antonioya)\\nMaintainer:\\nAntonio Vazquez (antonioya)\\nLicense:\\nGPL\\nSupport Level:\\nCommunity\\nNote:\\nThis add-on is bundled with Blender.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('### System:\\nYou are a helpful AI assistant.\\n\\n### Instruction:\\nTell me about the MeasureIt addon in Blender\\n\\n### Response:\\n* [Add-ons](../index.html)\\n* [3D View](index.html)\\n* MeasureIt\\n# MeasureIt\\nMeasureIt is an add-on designed for displaying measures in the viewport,\\nmaking the process of design objects with exact measures, easier. These tools\\nare extremely useful for any job that requires exact measurements, including\\narchitectural projects, technical design and 3D printing.\\n## Activation\\n* Open Blender and go to Preferences then the Add-ons tab.\\n* Click 3D View then MeasureIt to enable the script.\\n## Interface\\n### Overview\\nLocated in the 3D Viewport ‣ Sidebar ‣ View tab. The MeasureIt Tools panel is\\ndescribed below.\\nTo view the measures you need to press the _Show_ button. Many measure styles\\nappear grayed out in the menu, these are active in Edit Mode.\\n* The Mesh Debug sub panel has extra display options.\\n* The Items sub panel appears after adding a measure. This contains the color settings for each measure.\\n* The Configuration sub panel contains the font settings.\\nAs all measure definitions are saved in the blend-file, you can save the file\\nand the next time you use it, the measures will be ready.\\nReference\\nCategory:\\n3D View\\nDescription:\\nTools for measuring objects in the 3D Viewport.\\nLocation:\\n3D Viewport ‣ Sidebar ‣ View tab\\nFile:\\nmeasureit folder\\nAuthor:\\nAntonio Vazquez (antonioya)\\nMaintainer:\\nAntonio Vazquez (antonioya)\\nLicense:\\nGPL\\nSupport Level:\\nCommunity\\nNote:\\nThis add-on is bundled with Blender.\\n    ',\n",
       " '### System:\\nYou are a helpful AI assistant.\\n\\n### Instruction:\\nTell me about the MeasureIt addon in Blender\\n\\n### Response:\\n    ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_prompt = rag_prompt_sci(query, context)\n",
    "prompt = sci_prompt(query)\n",
    "\n",
    "rag_prompt, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the response using RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12266.27 ms\n",
      "llama_print_timings:      sample time =       8.67 ms /    76 runs   (    0.11 ms per token,  8770.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12265.99 ms /   436 tokens (   28.13 ms per token,    35.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3481.19 ms /    75 runs   (   46.42 ms per token,    21.54 tokens per second)\n",
      "llama_print_timings:       total time =   15866.37 ms /   511 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the response without RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12266.27 ms\n",
      "llama_print_timings:      sample time =       9.05 ms /    93 runs   (    0.10 ms per token, 10273.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    4120.08 ms /    93 runs   (   44.30 ms per token,    22.57 tokens per second)\n",
      "llama_print_timings:       total time =    4251.18 ms /    94 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a response using the RAG-augmented prompt\n",
    "print(\"Generating the response using RAG...\")\n",
    "rag_response = llm.generate_response(rag_prompt)\n",
    "\n",
    "# Compare with a standard response without RAG\n",
    "print(\"Generating the response without RAG...\")\n",
    "response = llm.generate_response(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### System:\\nYou are a helpful AI assistant.\\n\\n### Instruction:\\nTell me about the MeasureIt addon in Blender\\n\\n### Response:\\n    '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without RAG:\n",
      "2.How to use it:\n",
      "    - Open Blender and select the objects you want to measure.\n",
      "    -[Retrieval]<paragraph> Install MeasureIt_2.0b1_windows.exe\n",
      "    - Installation is simple, download and run the installer, then restart Blender</paragraph>[Relevant] - Open the MeasureIt add-on by going to \"Add-ons\" > \"MeasureIt\" in the main menu.[No support / Contradictory][Utility:4]\n"
     ]
    }
   ],
   "source": [
    "# Output the results with and without RAG\n",
    "print(f\"Response without RAG:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Response:\n",
      "4. Click the _Open Settings_ button to open the settings menu, where you can customize the appearance of the measurements, including font size, color, and transparency.\n",
      "## Features\n",
      "* Display of several measures directly in the 3D viewport.\n",
      "*[No Retrieval] Distance measuring between points (also diagonally).[No Retrieval] - Perpendicular distance from a\n"
     ]
    }
   ],
   "source": [
    "print(f\"RAG-Response:\\n{rag_response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "figma-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
